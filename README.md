# SAT-5165-small-project-3
In this project, we implemented five machine learning methods—Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, Random Forest, and Support Vector Machine (SVM)—to classify cardiovascular disease risk. Each algorithm was trained on a preprocessed dataset that involved encoding categorical variables and standardizing numerical features. Logistic Regression provided a straightforward baseline, using a linear model to estimate the probability of disease, while KNN classified individuals based on the majority class of their nearest neighbors. Decision Trees and Random Forests captured more complex patterns, with Random Forest improving performance by averaging multiple decision trees to reduce overfitting. SVM aimed to find an optimal decision boundary that maximizes the margin between classes, performing well in high-dimensional spaces.
To evaluate the performance, we used several key metrics: accuracy, AUC (Area Under the Curve), recall, and precision. The AUC metric helped assess each model’s ability to distinguish between patients with and without cardiovascular disease, with higher values indicating stronger performance. The recall and precision metrics were particularly valuable in cases of potential class imbalance, such as identifying rare disease cases. Overall, models like Random Forest and SVM showed high performance, leveraging their ability to handle complex patterns and non-linear relationships in the data. The findings suggest that ensemble methods like Random Forest might be particularly suitable for disease classification due to their robustness and ability to generalize well across varied data points.
In my project, Spark was essential for managing and processing a cardiovascular health dataset with over 11,000 records. Handling such a large dataset on a single machine was slow and challenging, but Spark’s distributed computing capabilities on two VMs made data preprocessing and statistical analysis much faster. Spark enabled me to address data quality issues, like missing values and inconsistent entries, by streamlining data cleaning processes. This enhanced data integrity, ensuring that the information used for analysis was reliable and accurate.
Spark also proved invaluable for computationally intensive tasks, such as correlation analysis and predictive modeling. Calculating correlations between cardiovascular risk factors and disease outcomes required significant processing power, and Spark’s parallel processing made this analysis efficient. For predictive modeling, Spark allowed me to train and test machine learning algorithms at scale, optimizing model performance and speeding up training times. Additionally, Spark’s scalability enabled me to compare performance across different configurations, demonstrating that using multiple nodes significantly reduced processing time and optimized resource utilization.
